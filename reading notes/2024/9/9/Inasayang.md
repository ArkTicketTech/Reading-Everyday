# 有监督回归-最小二乘学习法

回归是指把实函数在样本点附近加以近似的有监督的函数近似问题。

回归问题的最基本算法：最小二乘学习法 。

最小二乘学习法是对模型的输出  $f_\theta(x_i)$  和训练集输出 $\{ y_i \}_{i=1}^n$ 的平方误差

$$
J_{LS}(\theta) = \frac{1}{2} \sum_{i=1}^{n} \left( f_\theta(x_i) - y_i \right)^2
$$

为最小时的参数$\theta$进行学习 

$$
\hat{\theta}_{LS} = \arg\min_{\theta} J_{LS}(\theta)
$$

<aside>
💡

LS: Least Squares（最小二乘）是一种数学优化方法，用于找到数据的最佳拟合曲线或模型。其核心思想是通过最小化预测值与实际观测值之间的平方误差和，来确定模型参数。

</aside>

加上系数$1/2$, 是为了约去对$J_{LS}$进行微分时得到的2。
平方误差 $(f_\theta(x_i) - y_i)^2$  是残差

$$
f_\theta(x_i) - y_i
$$

的$l_2$范数(欧几里得范数)。

最小二乘学习法也称为$l_2$损失最小化学习法。

- Tips:
    1. **平方误差**：
        - 平方误差是指预测值与实际观测值之间差异的平方。对于一个预测值 $y^i$ 和实际值 $*yi*$，平方误差为 $(yi−y^i)2$。
        - 在最小二乘法中，目标是最小化所有观测值的平方误差和。
    2. **残差**：
        - 残差是指实际观测值与预测值之间的差异，即 $yi−y^i$。
        - 残差用于评估模型的拟合程度。较小的残差表示模型对数据的拟合较好。
    
    平方误差是残差的平方，因此平方误差总是非负的，并且放大了较大的误差。
    

如线性模型

$$
f_\theta(x) = \sum_{j=1}^{b} \theta_i \phi_i(x) = \theta^\top \phi(x)
$$

训练样本的平方差$J_{ls}$则为

$$
J_{LS}(\theta) = \frac{1}{2} \|\Phi \theta - \mathbf{y}\|^2
$$

$\mathbf{y} = (y_1, \cdots, y_n)^\top$ 是训练输出的 $n$ 维向量， $\Phi$ 是下式中定义的 $n \times b$ 阶矩阵(也称为设计矩阵)。

$$
\Phi = \begin{pmatrix} 
\phi_1(x_1) & \cdots & \phi_b(x_1) \\ 
\vdots & \ddots & \vdots \\ 
\phi_1(x_n) & \cdots & \phi_b(x_n) 
\end{pmatrix}
$$

参数向量的$\theta$的偏微分$\nabla_\theta J_{LS}$

$$
\nabla_\theta J_{LS} = \left( \frac{\partial J_{LS}}{\partial \theta_1}, \cdots, \frac{\partial J_{LS}}{\partial \theta_b} \right)^\top = \Phi^\top \Phi \theta - \Phi^\top \mathbf{y}
$$

将其微分设置为0，最小二乘解就满足关系式 

$$
\Phi^\top \Phi \theta = \Phi^\top \mathbf{y}
$$

$$
\hat{\theta}_{LS} = \Phi^{\dagger} \mathbf{y}
$$

在这里，$†$ 是剑标。相对于只有方阵、非奇异矩阵才能定义逆矩阵，广义逆矩阵则是矩形矩阵或奇异矩阵都可以定义，是对逆矩阵的推广。$\Phi^\top \Phi$ 有逆矩阵的时候，广义逆矩阵 $\Phi^{\dagger}$ 可以用下式表示。

$$
\Phi^{\dagger} = (\Phi^\top \Phi)^{-1} \Phi^\top
$$

对顺序为 $i$ 的训练样本的平方差通过权重 $w_i \geq 0$ 进行加权，然后再采用最小二乘法学习，又称加权最小二乘学习法。

$$
\min_{\theta} \frac{1}{2} \sum_{i=1}^{n} w_i (f_{\theta}(x_i) - y_i)^2
$$

$$
(\Phi^\top W \Phi)^{\dagger} \Phi^\top W y
$$

通过上式进行求解。

$W$ 是以 $w_1, \cdots, w_n$ 为对角元素的对角矩阵。

- 总结
    - **最小二乘学习法**
        - **目标**：最小化预测值与实际值之间的平方误差和。
        - **公式**：$\min_{\theta} \sum_{i=1}^{n} (f_{\theta}(x_i) - y_i)^2$
        - **应用**：适用于所有数据点具有相同重要性的情况。
    - **加权最小二乘学习法**
        - **目标**：最小化加权平方误差和，其中每个数据点的误差被一个权重放大或缩小。
        - **公式**：$\min_{\theta} \sum_{i=1}^{n} w_i (f_{\theta}(x_i) - y_i)^2$
        - **应用**：适用于不同数据点具有不同重要性的情况。例如，当某些数据点的测量误差较大时，可以赋予较小的权重。
    - **区别**
        - **权重**：加权最小二乘法为每个数据点分配一个权重，而普通最小二乘法假设所有权重相等。
        - **鲁棒性**：加权最小二乘法可以更好地处理异方差性（即不同数据点的误差方差不同）和异常值。
        - 加权最小二乘法通过调整权重来提高模型对特定数据点的适应性，是普通最小二乘法的推广。