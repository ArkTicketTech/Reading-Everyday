# 2-学习模型

- 线性模型
    
    最简单的模型就是线性模型$\theta * E$ 
    
    - $*θ*$ 是一个标量，表示模型的参数或系数，这个系数 $*θ*$ 反映了变量 $E$ 对模型输出的影响程度
    - $*E*$ 是一个变量或特征
    
    因为这个模型只能表现线性的输人输出函数（即直线关系），所以在解决实际问题方面，往往没有太大的实用价值 。
    
    在实际应用中，经常会对线性模型进行相应的扩展,使其变成基于参数的线性模型. 这样就可以使线性模型也能用于表示非线性的输入与输出了 。
    
    $$
    f_\theta(x) = \sum_{j=1}^{b} \theta_j \phi_j(x) = \theta^\top \phi(x)
    $$
    
    $\phi_j(x)$是基函数向量 $\phi(x) = (\phi_1(x), \cdots, \phi_b(x))^\top$ 的第 $j$ 个因子，$\theta_j$ 是参数向量 $\theta = (\theta_1, \cdots, \theta_b)^\top$的第 $j$ 个因子。另外，$b$ 是基函数的个数，上标 $^\top$ 表示矩阵的转置。
    
    如果把基函数变为多项式的形式 
    
    $$
    \phi(x) = (1, x, x^2, \ldots, x^{b-1})^\top
    $$
    
    线性模型就可以表示复杂的非线性模型了 
    一维的输入$x$还可以扩展为$d$维的向量形式 。
    对于多维的输入向量$x$，关键是选择合适的基函数 。
    
    ***乘法模型***是指，把一维的基函数作为因子，通过使其相乘而获得多维基函数的方法。 
    
    $$
    f_\theta(x) = \sum_{j_1=1}^{b'} \cdots \sum_{j_d=1}^{b'} \theta_{j_1, \ldots, j_d} \phi_{j_1}(x^{(1)}) \cdots \phi_{j_d}(x^{(d)})
    $$
    
    $b'$代表各维的参数个数 
    所有参数的个数是$（b')^d$, 即总的输人维数是以$d$次方的形式呈指数级增长的 ，当$b'$=10,$d$是100的时候，全部参数的个数将会是 
    
    $$
    10^{100} = 1\underbrace{000\cdots000}_{100 \text{个}}
    $$
    
    随着维数的增加、计算量呈指数级增长的现象，通常称为维数灾难。
    
    ***加法模型***是指，把一维的基函数作为因子，通过使其相加而获得多维基函数的方法 。
    
    $$
    f_\theta(x) = \sum_{k=1}^{d} \sum_{j=1}^{b'} \theta_{k,j} \phi_j(x^{(k)})
    $$
    
    ***加法模型***中所有参数的个数是$b'd$. 其只会随着输入维数$d$呈线性增长。当$b'$=10, $d$是100的时候，全部参数的个数是$10 * 100 = 1000$ 。
    
    <aside>
    💡
    
    ***乘法模型***的表现力非常丰富，但是参数个数会随着输入维数$d$呈指数级增加。另一方面，
    
    虽然***加法模型***的参数个数是随者输入维数$d$ 呈线性增加的，但其表现力又相对较弱。
    
    </aside>
    
- 核模型
    
    核模型是基于一个被称为核函数的二元函数$K(\cdot, \cdot)$
    
    通过核函数$K(x, x_j)_{j=1}^n$ 的线性组合来定义模型,其中$*x*$和$*x_j*$ 是输入数据点
    
    利用核函数来处理数据，使得在高维空间中可以应用线性方法来解决非线性问题
    
    $$
    f_{\theta}(x) = \sum_{j=1}^{n} \theta_j K(x, x_j)
    $$
    
    在众多的核函数中，以高斯核函数的使用最为广泛 
    
    $$
    K(x, c) = \exp\left(-\frac{\|x - c\|^2}{2h^2}\right)
    $$
    
    $\|\cdot\| \text{ 表示 2 范数，即 } \|x\| = \sqrt{x^T x}$
    
    $h$和$c$分别对应于高斯核函数的带宽与均值
    
    
    <aside>
    💡
    
    2 范数，也称为欧几里得范数，是向量长度的一种度量方式。对于一个向量$x = (x_1, x_2, \ldots, x_n)$，其 2 范数定义为
    
    $$
    \|x\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
    $$
    
    它表示从原点到点 *x* 的直线距离。在矩阵的情况下，2 范数通常指的是矩阵的谱范数，即最大奇异值。
    
    </aside>
    
    在高斯核函数中，对各个输入样本$\{x_i\}_{i=1}^n$进行高斯核配置 ，并把其高度$\{\theta_i\}_{i=1}^n$作为参数进行学习，一般只能在训练集的输入样本附近对函数进行近似 。
    
    因为高斯核函数具有局部性特征，其值在距离中心较远的地方迅速衰减。
    模型在训练样本附近能够很好地拟合数据，但在远离训练样本的区域，模型的预测可能不够准确。
    
    这种特性意味着高斯核模型在训练集的输入样本附近能够有效地捕捉数据的复杂模式，但在未见过的数据点或远离训练样本的区域，模型的泛化能力可能会下降。
    
    高斯核模型在一定程度上减轻了维数灾难的影响，因为它主要关注训练样本附近的局部区域，而不是整个高维空间。
    
    
    
    在核模型里，参数的个数不依赖于输入变量$x$的维数$d$,只由训练样本数$n$决定 。
    
    如果样本数很大，也可以去部分集合作为核均值来计算。
    
    核模型是参数向量 $\theta = (\theta_1, \ldots, \theta_n)^\top$ 的线性形式 。
    
    通常把与基于参数的线性模型称为参数模型，把核模型称为非参数模型。
    
- 层级模型（TODO）