## Chapter 10: Batch Processingæ‰¹å¤„ç†ç³»ç»Ÿ

As usual, history has a tendency of repeating itself.

> the ideas and lessons from Unix carry over to large-scale, heterogeneous distributed data systems.
>
> UNIXçš„æ€æƒ³å’Œç»éªŒæ™®éè§äºå¤§è§„æ¨¡ã€å¼‚æ„åˆ†å¸ƒå¼æ•°æ®ç³»ç»Ÿã€‚

### Batch Processing with Unix Tools

#### Simple Log analysisç®€å•æ—¥å¿—åˆ†æ

```shell
cat /var/log/nginx/access.log |
awk '{print $7}' |
sort |
uniq -c |
sort -r -n |
head -n 5
```

`in a matter of seconds` åœ¨å‡ ç§’é’Ÿå†…

##### Chains of commands versus custom program

a simple program to do the same thing

```ruby
counts=Hash.new(0)

File.open('/var/log/nginx/access.log') do |file|
  file.each do |line|
    url=line.split[6]
    counts[url]+=1
  end
end

top5=counts.map{|url,count| [count,url]}.sort.reverse[0...5]
top5.each{|count,url| puts "#{count} #{url}"}
```

##### Sorting versus in-memory aggregation

æ’åº VS å†…å­˜èšåˆ

æ’åºï¼š

* UNIXæµæ°´çº¿
* job's working set is larger than the available memory: make efficient use of disks é«˜æ•ˆåœ°ä½¿ç”¨ç£ç›˜
* SSTables and LSM-Tables
  * chunks of data can be sorted in memory 
  * written out to disk as segment files
  * multiple sorted segments can be merged into a larger sorted file

* Merge-sort has sequential access patterns that perform well on disks. ? ä¸å¤ªç†è§£è¿™å¥ã€‚

å†…å­˜èšåˆï¼š

* Rubyè„šæœ¬ä½¿ç”¨ä¸€ä¸ªURLçš„å†…å­˜å“ˆå¸Œè¡¨
* é€‚ç”¨äºå¤§å¤šæ•°ä¸­å°å‹ç½‘ç«™ï¼ˆå–å†³äºURLçš„ä¸åŒä¸ªæ•°ï¼‰

å‘ç°ä¸­æ–‡ç¿»è¯‘çš„ç¡®ä¸å¤ªå¥½ã€‚ä¸­æ–‡å‰¯æ ‡é¢˜ä¸ºâ€œæ’åºä¸å†…å­˜èšåˆâ€ï¼Œåˆšå¼€å§‹è¿˜æ²¡ååº”è¿‡æ¥è¿™å…¶å®æ˜¯å¯¹æ—¥å¿—æ–‡ä»¶çš„ä¸¤ç§ä¸åŒå¤„ç†æ–¹å¼ã€‚å»ç¿»è‹±æ–‡ç‰ˆï¼Œå‘ç°æ ‡é¢˜ä¸º`Sorting versus in-memory aggregation`ï¼Œä½¿ç”¨äº†`versus`,é©¬ä¸Šå¯ä»¥ä½“ä¼šåˆ°å¯¹æ—¥å¿—åˆ†æçš„ä¸åŒå¤„ç†æ–¹æ³•ã€‚è¿˜æ˜¯è¯»è‹±æ–‡ç‰ˆå§ï¼Œæ¯•ç«Ÿæ˜¯ç¬¬ä¸€æ‰‹è€Œä¸æ˜¯åˆç»è¿‡å¦ä¸€ä¸ªäººç¿»è¯‘æœ‰åå·®çš„äºŒæ‰‹èµ„æ–™ã€‚é˜®ä¸€å³°ç¿»è¯‘å¥½çš„ä¸€éƒ¨åˆ†åŸå› æ˜¯ä»–ä¼šä¿ç•™ä¸€äº›è‹±æ–‡è¡¨ç¤ºï¼Œå¯ä»¥ç†è§£ä¸ºè¯¥ä¹¦ç¿»è¯‘æ˜¯ä¸ºäº†æ›´å¤šè¯»è€…å¯ä»¥çœ‹æ‡‚ï¼Œä½†versusä½¿ç”¨ä¸€ä¸ªâ€œä¸â€ä»£æ›¿åè€Œå¤±æ‰äº†å‡†ç¡®æ€§ğŸ˜…ã€‚

#### The Unix Philosophy UNIXè®¾è®¡å“²å­¦

##### A uniform interface ç»Ÿä¸€æ¥å£

interface: fileæ–‡ä»¶ï¼ˆfile Descriptoræ–‡ä»¶æè¿°ç¬¦) 

A file is just an ordered sequence of bytes.

* an actual file on the filesystem
* a `communication channel` to`another process`(Unix socket, stdin, stdout)
* a device driver (say /dev/audio or /dev/Ip0)
* a socket representating a TCP connection

##### Separation of logic and wiring é€»è¾‘ä¸å¸ƒçº¿åˆ†ç¦»

æ¯”å¦‚è¿™ä¸ªï¼Œè‹±æ–‡ä¸ºâ€œSeparation of logic and wiringâ€ï¼Œä¹Ÿæ˜¯å¾ˆæ˜æ˜¾è‹±æ–‡å®¹æ˜“ç†è§£ï¼Œå½“æ—¶è¯»ä¸­æ–‡å°±åœ¨æƒ³è¿™ä»€ä¹ˆæ„æ€ã€‚

stdinã€stdout

pros: 

* loose couplingæ¾è€¦åˆ/late bindingåæœŸç»‘å®š/inversion of controlæ§åˆ¶åè½¬

cons: 

* ä¸åˆ©äºå¤šä¸ªè¾“å…¥è¾“å‡ºçš„ç¨‹åº Programs that need multiple inputs or outputs are possible but tricky. ï¼Ÿä¸å¤ªæ‡‚
* Can't pipe a program's output into a network connection.  ï¼Ÿä¸å¤ªæ‡‚

##### Transparentcy and experimentation é€æ˜ä¸æµ‹è¯•

### MapReduct and Distributed Filesystems

> æœ‰æ„æ€çš„ç±»æ¯”ï¼š`MapReduceæœ‰ç‚¹åƒåˆ†å¸ƒåœ¨æ•°åƒå°æœºå™¨ä¸Šçš„UNIXå·¥å…·ã€‚`

A single MapReduce job is comparable to a single Unix process.

* takes one/more inputs
* produces one/more outputs: 
  * are written once in a sequential fashion
  * not modifying any existing part of a file once it has been written

#### HDFS

shard-nothing approach: HDFS

* requires no special hardware 
* only computers connected by a `conventional datacenter network`.

shard-disk approach

* Network Attached Storage(NAS)
* Storage Area Network(SAN)
* implemented by a `centralized storage appliance`é›†ä¸­å­˜å‚¨è®¾å¤‡, often using custom hardware and special network `infrastructure` such as Fibre Channel

`NameNode`: a center server keeps track of which file blocks are stored on which machine.

#### MapReduce Job Execution

The sorting is performed in stages. åˆ†é˜¶æ®µè¿›è¡Œæ’åºã€‚

MapReduce discards the partial output of a `failed job`.



